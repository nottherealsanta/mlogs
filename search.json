[
  {
    "objectID": "todo.html",
    "href": "todo.html",
    "title": "mlogs",
    "section": "",
    "text": "change dataframe font size in css\nposts/why scale?\nposts/notebook_formatter\napp/metrics - how TP and FP along with Threshold affect metrics\nIdeas/Feature_store\npost/auc_vs_ks\ntools/local_pypi\ninfra/JupyterHub\ninfra/Ray\ninfra/Metabase\nKS/ks_curve\nalgo/ Trees / Decision, Random, Extra, Gradient, XG and Ada\n\npruning - pre and post"
  },
  {
    "objectID": "posts/memory-profiler/memory_profiler.html",
    "href": "posts/memory-profiler/memory_profiler.html",
    "title": "Memory Profiler",
    "section": "",
    "text": "Understand memory usage of your Python code.\n\n\npip install memory_profiler\n\n\n\nExample Script :\n> cat memory_profiler_sample.py\nimport numpy as np\nimport pandas as pd\ndf = pd.DataFrame(np.random.randint(0,100,size=(100, 4)), columns=list('ABCD'))\n\n\nmprof run memory_profiler_sample.py\nmprof run --interval 1 memory_profiler_sample.py\nmprof run -o output.dat memory_profiler_sample.py\n\n\n\n\n\n\nTip\n\n\n\nUse mprof peak <executable> to find the peak memory usage\n\n\n\n\nmprof plot\n\n\n\nremoves .dat files\nmprof clean\n\n\n\n\n\n\nNote\n\n\n\nUnder the hood, mprof uses psutil to get the memory usage of the process. This means that the memory usage reported by mprof is the same as the one reported by psutil.Process().memory_info(). This is different from the memory usage reported by top or htop which is the memory usage of the process plus the memory usage of its children. This is why mprof reports a lower memory usage than top or htop.\n\n\n\n\n\n\n\n\nNote\n\n\n\nmprof can be run separately. Use mprof attach <pid> to attach to a running process."
  },
  {
    "objectID": "posts/optuna/optuna.html",
    "href": "posts/optuna/optuna.html",
    "title": "Optuna",
    "section": "",
    "text": "Optuna automates hyperparameter optimization.\n\n\n\n\nimpertive and define-by-run code style is great!\nalgorithm agnostic - optimize anything\n\n\n\n\nThe knobs ðŸŽ›ï¸ that dictate how machines learn. Knobs like: length of training, maximum depth of tree and strength of regularization.\nEvery type of machine learning algorithm has its own set of hyperparameters. The norm till now has been to get a intitutive sense of what hyperparameters to use for different types of problems and different datasets; or do a grid-search.\n\n\n\nSequentially selecting different hyperparameter sets where next set is selected by Bayesian reasoning (dependent on the previous runs).\nAs a stark difference to grid-search where we already have a set of hyperparameters to search, instead we look at the previously selected hyperparameters and itâ€™s results to intelligently select the next set of hyperparameters to search.\n\n\n\n# pip\npip install optuna\n# conda\nconda install -c conda-forge optuna\n\n\n\n\nimport optuna\n\n\n\n\n# imports\nimport pandas as pd\nfrom sklearn.datasets import make_classification\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score\nfrom sklearn.model_selection import train_test_split\n\n\n# make sample dataset\nX, y = make_classification(class_sep=0.5, random_state=0)\nX[:2], y[:2]\n\n(array([[-0.03926799,  0.13191176, -0.21120598, -0.7141474 ,  0.89992843,\n         -0.42132759,  0.76877064,  0.87569568,  2.45122972, -0.48024204,\n         -1.42296498, -0.52325094,  0.70869527, -1.98056559, -1.36516288,\n         -0.94893281,  1.94709864,  1.47698901,  1.02122474, -0.46931074],\n        [ 0.77416061,  0.10490717, -0.33281176, -0.77034986,  0.22899659,\n         -0.82282832, -0.61262574,  1.49613964,  0.56845892, -0.46749317,\n         -1.09203185, -0.8624933 , -0.63119194,  0.13391292, -0.97240289,\n         -0.77445897,  1.34622107,  0.7678044 ,  0.62251914, -1.49026539]]),\n array([0, 0]))\n\n\n\n# split into train-test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.33, random_state=42\n)\n\n\n# without optuna\nmodel = LogisticRegression(random_state=23)\nmodel.fit(X_train, y_train)\n\nprint(f\"Train F1 Score : {f1_score(y_train, model.predict(X_train))}\")\nprint(f\"Test  F1 Score : {f1_score(y_test, model.predict(X_test))}\")\n\nTrain F1 Score : 0.7761194029850745\nTest  F1 Score : 0.742857142857143\n\n\n\n# objective function\ndef objective(trial):\n    params = {\n        \"C\": trial.suggest_loguniform(\"C\", 1e-4, 1e1),\n        \"penalty\": trial.suggest_categorical(\"penalty\", [\"l2\", \"l1\", \"elasticnet\"]),\n        \"solver\": \"saga\",\n        \"random_state\": 0,\n        \"n_jobs\": 14,\n    }\n    # you can have conditional params. \n    if params[\"penalty\"] == \"elasticnet\":\n        params[\"l1_ratio\"] = trial.suggest_loguniform(\"l1_ratio\", 1e-3, 1)\n\n    model = LogisticRegression(**params)\n    model.fit(X_train, y_train)\n    \n    # return the value to be optimized\n    return f1_score(y_test, model.predict(X_test))\n\n# optuna's study - set direction of optimization \nstudy = optuna.create_study(direction=\"maximize\")\n\n# something like model.fit()\nstudy.optimize(objective, n_trials=20)\n\n[I 2022-04-25 11:02:02,189] A new study created in memory with name: no-name-d513816f-f254-49a1-a1dc-4999f55050eb\n[I 2022-04-25 11:02:02,237] Trial 0 finished with value: 0.0 and parameters: {'C': 0.001035440904319486, 'penalty': 'elasticnet', 'l1_ratio': 0.0168047681115154}. Best is trial 0 with value: 0.0.\n[I 2022-04-25 11:02:02,250] Trial 1 finished with value: 0.0 and parameters: {'C': 0.0008908409534393241, 'penalty': 'elasticnet', 'l1_ratio': 0.015358624475662915}. Best is trial 0 with value: 0.0.\n[I 2022-04-25 11:02:02,269] Trial 2 finished with value: 0.653061224489796 and parameters: {'C': 0.00016370922155922157, 'penalty': 'l2'}. Best is trial 2 with value: 0.653061224489796.\n[I 2022-04-25 11:02:02,309] Trial 3 finished with value: 0.0 and parameters: {'C': 0.0009138573600528666, 'penalty': 'elasticnet', 'l1_ratio': 0.3449491818274051}. Best is trial 2 with value: 0.653061224489796.\n[I 2022-04-25 11:02:02,337] Trial 4 finished with value: 0.742857142857143 and parameters: {'C': 0.33000390588332, 'penalty': 'l2'}. Best is trial 4 with value: 0.742857142857143.\n[I 2022-04-25 11:02:02,710] Trial 5 finished with value: 0.0 and parameters: {'C': 0.004900173758953741, 'penalty': 'elasticnet', 'l1_ratio': 0.5739239633308959}. Best is trial 4 with value: 0.742857142857143.\n[I 2022-04-25 11:02:02,725] Trial 6 finished with value: 0.0 and parameters: {'C': 0.0008523376996942561, 'penalty': 'l1'}. Best is trial 4 with value: 0.742857142857143.\n[I 2022-04-25 11:02:02,739] Trial 7 finished with value: 0.7272727272727272 and parameters: {'C': 0.004253229678729536, 'penalty': 'elasticnet', 'l1_ratio': 0.005630194052029165}. Best is trial 4 with value: 0.742857142857143.\n[I 2022-04-25 11:02:02,749] Trial 8 finished with value: 0.7058823529411765 and parameters: {'C': 0.004046243863102824, 'penalty': 'elasticnet', 'l1_ratio': 0.0017367022371214882}. Best is trial 4 with value: 0.742857142857143.\n[I 2022-04-25 11:02:02,764] Trial 9 finished with value: 0.7272727272727272 and parameters: {'C': 0.03109891412503484, 'penalty': 'l2'}. Best is trial 4 with value: 0.742857142857143.\n[I 2022-04-25 11:02:02,820] Trial 10 finished with value: 0.7058823529411765 and parameters: {'C': 7.8208518561020846, 'penalty': 'l2'}. Best is trial 4 with value: 0.742857142857143.\n[I 2022-04-25 11:02:02,867] Trial 11 finished with value: 0.7647058823529411 and parameters: {'C': 0.9355154190877125, 'penalty': 'l1'}. Best is trial 11 with value: 0.7647058823529411.\n[I 2022-04-25 11:02:02,910] Trial 12 finished with value: 0.7647058823529411 and parameters: {'C': 1.3562536567204646, 'penalty': 'l1'}. Best is trial 11 with value: 0.7647058823529411.\n[I 2022-04-25 11:02:02,957] Trial 13 finished with value: 0.7647058823529411 and parameters: {'C': 1.6829408470804865, 'penalty': 'l1'}. Best is trial 11 with value: 0.7647058823529411.\n[I 2022-04-25 11:02:02,992] Trial 14 finished with value: 0.8125 and parameters: {'C': 0.27325083947591605, 'penalty': 'l1'}. Best is trial 14 with value: 0.8125.\n[I 2022-04-25 11:02:03,040] Trial 15 finished with value: 0.8666666666666666 and parameters: {'C': 0.14326745204868416, 'penalty': 'l1'}. Best is trial 15 with value: 0.8666666666666666.\n[I 2022-04-25 11:02:03,078] Trial 16 finished with value: 0.8666666666666666 and parameters: {'C': 0.11051572179107788, 'penalty': 'l1'}. Best is trial 15 with value: 0.8666666666666666.\n[I 2022-04-25 11:02:03,133] Trial 17 finished with value: 0.653061224489796 and parameters: {'C': 0.06645745504510152, 'penalty': 'l1'}. Best is trial 15 with value: 0.8666666666666666.\n[I 2022-04-25 11:02:03,182] Trial 18 finished with value: 0.653061224489796 and parameters: {'C': 0.05066087235412606, 'penalty': 'l1'}. Best is trial 15 with value: 0.8666666666666666.\n[I 2022-04-25 11:02:03,227] Trial 19 finished with value: 0.7999999999999999 and parameters: {'C': 0.1937529842391572, 'penalty': 'l1'}. Best is trial 15 with value: 0.8666666666666666.\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe above cell outputs logs from Optuna. Jupyter thinks these are errors and displays them in red. Ignore this.\n\n\n\n# best trail\nstudy.best_params, study.best_value\n\n({'C': 0.14326745204868416, 'penalty': 'l1'}, 0.8666666666666666)\n\n\n\n# see trails sorted by objective values\nstudy.trials_dataframe().sort_values('value',ascending=False).head()\n\n\n\n\n\n  \n    \n      \n      number\n      value\n      datetime_start\n      datetime_complete\n      duration\n      params_C\n      params_l1_ratio\n      params_penalty\n      state\n    \n  \n  \n    \n      16\n      16\n      0.866667\n      2022-04-25 11:02:03.042372\n      2022-04-25 11:02:03.078160\n      0 days 00:00:00.035788\n      0.110516\n      NaN\n      l1\n      COMPLETE\n    \n    \n      15\n      15\n      0.866667\n      2022-04-25 11:02:02.995554\n      2022-04-25 11:02:03.039449\n      0 days 00:00:00.043895\n      0.143267\n      NaN\n      l1\n      COMPLETE\n    \n    \n      14\n      14\n      0.812500\n      2022-04-25 11:02:02.961086\n      2022-04-25 11:02:02.991294\n      0 days 00:00:00.030208\n      0.273251\n      NaN\n      l1\n      COMPLETE\n    \n    \n      19\n      19\n      0.800000\n      2022-04-25 11:02:03.187038\n      2022-04-25 11:02:03.226408\n      0 days 00:00:00.039370\n      0.193753\n      NaN\n      l1\n      COMPLETE\n    \n    \n      11\n      11\n      0.764706\n      2022-04-25 11:02:02.823591\n      2022-04-25 11:02:02.866632\n      0 days 00:00:00.043041\n      0.935515\n      NaN\n      l1\n      COMPLETE\n    \n  \n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nBest way to search for optimial hyperparameters is to cross-validate.\n\n\n\n\n\nOptuna works with in-memory storage (like in the example above). It is also possible to save and resume from RDB Backend.\n# creates a table in the db with the study_name as table name\nstudy = optuna.create_study(study_name=\"my_study\", storage=\"sqlite:///example.db\")\nstorage can point to any RDB system.\nstudy = optuna.create_study(study_name=\"my_study\", storage=\"mysql://root@localhost/example\")\n\n\n\nIt is easy to parallelize Optuna. You simply point to the same storage and run the script multiple time. Here is an link to optunaâ€™s documentation on this. The video on this page is a must watch, itâ€™s awesome.\nEach trials is independent of each other, therefore the only requirements for each trials is the set of previous trials and itâ€™s resultant objective value.\n\n\n\n\n\n\nTip\n\n\n\nYou can load the study while you have the optimization script(s) running to see the results in real-time.\n\n\nloaded_study = optuna.load_study(study_name=\"my_study\", storage=\"sqlite:///example.db\")\n\n\n\nSearch different model types in the same objective function.\ndef objective(trial):\n    \n    model_type = trial.suggest_categorical('model_type', ['logistic-regression','svm'])\n\n    if model_type == 'svm':\n        kernel = trial.suggest_categorical('kernel', ['linear', 'poly', 'rbf', 'sigmoid'])\n        model = SVC(kernel=kernel, C=regularization, degree=degree)\n        \n    if model_type == 'logistic-regression':\n        penalty = trial.suggest_categorical('penalty', ['l2', 'l1'])\n        model = LogisticRegression(penalty=penalty, C=regularization, solver=solver)\n        \n    ...\n\n\n\nOptuna use plotly to visualize trails. Here is the list of visualization available out of the box.\n\n\n\n\n\n\nWarning\n\n\n\nplotly must be installed for the following code to work. Install with pip install plotly\n\n\nfig = optuna.visualization.plot_optimization_history(study)\nfig.show()\n\n\n\n\nSamplers are the method Optuna uses to choose the next hyperparameters to search.\nAmong the samplers are :\n- TPE : Bayesian optimization based on kernel fitting\n- GP : Bayesian optimization based on Gaussian processes\n- CMA-ES : meta-heuristic algorithm for continuous space\nTPE is the default sampler (for single objective value)\n\n\n\n\n\n\nNote\n\n\n\nIt is possible to optimize for multiple values and have directions for each. This may be useful if you are trying to reduce overfit but increase testâ€™s set performance.\n\n\n\n\n\n\n\n\nparams = {\n    \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log=True) ,\n    \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10.0, log=True)  ,\n    \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 2**8 ),\n    \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.4, 1.0 ),\n    \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.4, 1.0),\n    \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n    \"min_child_samples\":trial.suggest_int(\"min_child_samples\", 5, 100),\n}\n\n\n\n    # https://raw.githubusercontent.com/abhishekkrthakur/autoxgb/main/src/autoxgb/params.py\n    params = {\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-2, 0.25, log=True),\n        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-8, 100.0, log=True),\n        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-8, 100.0, log=True),\n        \"subsample\": trial.suggest_float(\"subsample\", 0.1, 1.0),\n        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.1, 1.0),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 1, 9), \n        \"early_stopping_rounds\": trial.suggest_int(\"early_stopping_rounds\", 100, 500),\n        \"n_estimators\": trial.suggest_categorical(\"n_estimators\", [7000, 15000, 20000]),\n    }\n    if use_gpu: # if training on GPU\n        params[\"tree_method\"] = \"gpu_hist\"\n        params[\"gpu_id\"] = 0\n        params[\"predictor\"] = \"gpu_predictor\"\n    else:\n        params[\"tree_method\"] = trial.suggest_categorical(\"tree_method\", [\"exact\", \"approx\", \"hist\"])\n        params[\"booster\"] = trial.suggest_categorical(\"booster\", [\"gbtree\", \"gblinear\"])\n        if params[\"booster\"] == \"gbtree\":\n            params[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True)\n            params[\"grow_policy\"] = trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n\n\n\n\nAs we move to understanding more about data-centric AI, we are automating away the model-centric part which has fascinated us for a while now. Optuna is a step in this process."
  },
  {
    "objectID": "posts/KS/ks.html",
    "href": "posts/KS/ks.html",
    "title": "KS Metric",
    "section": "",
    "text": "Kolmogorov-Smirnov metric (ks metric) is derived from K-S test. K-S test measures the distance between two plotted cumulative distribution functions (CDF).\nTo use it as a metric for classification problem we see the distance of plotted CDF of target and non-target. It can be also defined as the maximum distance between TNR and FNR. ðŸ”—\nThe model that produces the greatest amount of separability between target and non-target distribution would be considered the better model.\n\n\n\n\nScale-invariant. It measures how well predictions are ranked, rather than their absolute values.\nThreshold-invariant. It measures the quality of the modelâ€™s predictions irrespective of what classification threshold is chosen.\nAlthough KS is a single metric. KS table / Gains Table can be used to take different actions for different bins.\n\n\n\n\nThe following steps can be also done with pandas \n\n# sample dataset and sample model\n\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nfrom sklearn.datasets import make_classification\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\n# make sample dataset\nX, y = make_classification(\n    n_samples=1000, class_sep=0.3, random_state=0\n)  # 1000 samples with 20 features\n\n# split train-test\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.33, random_state=42\n)\n\n# simple Logistic Model\nmodel = LogisticRegression(random_state=23)\nmodel.fit(X_train, y_train)\n\n# predictions\ny_train_pred = model.predict_proba(X_train)[:, 1]\ny_test_pred = model.predict_proba(X_test)[:, 1]\n\nprint(\n    X_train.shape,\n    y_train.shape,\n    y_train_pred.shape,\n    X_test.shape,\n    y_test.shape,\n    y_test_pred.shape,\n)\n\n(670, 20) (670,) (670,) (330, 20) (330,) (330,)\n\n\nConsider the following example, y_test represents true target values, here as 1s and 0s. and y_test_pred as prediction probabilities.\n\ny_test[:5], y_test_pred[:5]\n\n(array([0, 0, 1, 0, 1]),\n array([0.3143867 , 0.13123316, 0.15998113, 0.35006658, 0.83034574]))\n\n\n\n\n\nks_arr = np.dstack((y_test, y_test_pred))[0]  # dstack returns a 3D array with len(z-axis)=1\n\nks_arr[:5]  # view array\n\narray([[0.        , 0.3143867 ],\n       [0.        , 0.13123316],\n       [1.        , 0.15998113],\n       [0.        , 0.35006658],\n       [1.        , 0.83034574]])\n\n\n\n\n\n\nks_arr = ks_arr[np.argsort(ks_arr[:, 1], kind=\"stable\")]\n\nks_arr[:5]\n\narray([[0.        , 0.0355004 ],\n       [0.        , 0.04975776],\n       [0.        , 0.05752943],\n       [1.        , 0.06790324],\n       [0.        , 0.07714446]])\n\n\n\n\n\n\n# np.unique(y_test, return_counts=True) -> (array([0, 1]), array([158, 172]))\ny_true_counts = np.unique(y_test, return_counts=True)[1]\n\nN = y_true_counts[0]\nP = y_true_counts[1]\n\nprint(N, P)\n\n158 172\n\n\n\n\n\n\ncs_N = ((1 - ks_arr[:, 0]).cumsum() / N) * 100 # tnr\ncs_P = ((ks_arr[:, 0]).cumsum() / P) * 100 # fnr \n\ncs_N[:5], cs_P[:5]\n\n(array([0.63291139, 1.26582278, 1.89873418, 1.89873418, 2.53164557]),\n array([0.        , 0.        , 0.        , 0.58139535, 0.58139535]))\n\n\n\n# plot cs_N and cs_P\nfig, ax = plt.subplots(1, 1)\nax.plot(ks_arr[:, 1], cs_N)  # x-axis as y_pred\nax.plot(ks_arr[:, 1], cs_P)\nplt.show()\n\n\n\n\n\n\n\n\ncs_diff = np.abs(cs_N - cs_P)\n\ncs_diff[:5]\n\narray([0.63291139, 1.26582278, 1.89873418, 1.31733883, 1.95025022])\n\n\n\nks = cs_diff.max()\n\nks\n\n46.916396820724174\n\n\n\n\n\n\nfig, ax = plt.subplots(1, 1)\n# plot cumilative percent\nax.plot(ks_arr[:, 1], cs_N, label=\"Class 0\")\nax.plot(ks_arr[:, 1], cs_P, label=\"Class 1\")\n\n# plot max seperation\nks_threshold = ks_arr[cs_diff.argmax(), 1]\nax.axvline(\n    x=ks_threshold,\n    ymin=sorted([cs_P[cs_diff.argmax()] / 100, cs_N[cs_diff.argmax()] / 100])[0],\n    ymax=sorted([cs_P[cs_diff.argmax()] / 100, cs_N[cs_diff.argmax()] / 100])[1],\n    c=\"black\",\n    linewidth=1,\n    linestyle=\"--\",\n    label=f\"KS: {ks.round(2)} at {ks_threshold.round(3)}\",\n)\n\n# plot settings\nax.set_xlim([0, 1])\nax.set_ylim([0, 100])\nax.set_xlabel(\"Threshold\")\nax.set_ylabel(\"Percentage below threshold\")\nax.set_xticks(np.linspace(0, 1, 11))\nax.grid()\nax.legend()\n\nplt.show()\n\n\n\n\n\n\n\n\nThere is a strong relationship between AUC and KS.\n\nfrom sklearn.metrics import roc_curve\n\nfpr, tpr, thresholds = roc_curve(y_test, y_test_pred)\n\n(((tpr-fpr).max())*100).round(2), ks.round(2)\n\n(46.92, 46.92)\n\n\nExplore this Streamlit App to understand more about the relationship between them. Also refer to Youdenâ€™s J statistic.\n\n\n\n\n\n\n# dataframe with y_true and y_pred\ndf = pd.DataFrame()\ndf[\"score\"] = y_test_pred\n# one hot encoding the dependent variable\ndf[\"class_n\"] = 1 - y_test\ndf[\"class_p\"] = y_test\n\n# decile bins of prediction scores after ordering them\ndf[\"bin\"] = pd.qcut(df.score.rank(method=\"first\"), 10, labels=list(range(0, 10, 1)))\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      score\n      class_n\n      class_p\n      bin\n    \n  \n  \n    \n      0\n      0.314387\n      1\n      0\n      1\n    \n    \n      1\n      0.131233\n      1\n      0\n      0\n    \n    \n      2\n      0.159981\n      0\n      1\n      0\n    \n    \n      3\n      0.350067\n      1\n      0\n      2\n    \n    \n      4\n      0.830346\n      0\n      1\n      8\n    \n  \n\n\n\n\n\n\n\n\ngrouped = df.groupby(\"bin\", as_index=False)\n\nks_table = pd.DataFrame()\nks_table[\"min_score\"] = grouped.min().score\nks_table[\"max_score\"] = grouped.max().score\nks_table[\"n_class_n\"] = grouped.sum().class_n\nks_table[\"n_class_p\"] = grouped.sum().class_p\nks_table[\"n_total\"] = ks_table.n_class_n + ks_table.n_class_p\n\nks_table\n\n\n\n\n\n  \n    \n      \n      min_score\n      max_score\n      n_class_n\n      n_class_p\n      n_total\n    \n  \n  \n    \n      0\n      0.035500\n      0.224967\n      21\n      12\n      33\n    \n    \n      1\n      0.226262\n      0.327676\n      27\n      6\n      33\n    \n    \n      2\n      0.328646\n      0.377685\n      19\n      14\n      33\n    \n    \n      3\n      0.381554\n      0.437930\n      25\n      8\n      33\n    \n    \n      4\n      0.442221\n      0.491976\n      23\n      10\n      33\n    \n    \n      5\n      0.495512\n      0.575724\n      17\n      16\n      33\n    \n    \n      6\n      0.577944\n      0.671211\n      9\n      24\n      33\n    \n    \n      7\n      0.671344\n      0.757899\n      6\n      27\n      33\n    \n    \n      8\n      0.765076\n      0.874251\n      4\n      29\n      33\n    \n    \n      9\n      0.874542\n      0.988560\n      7\n      26\n      33\n    \n  \n\n\n\n\n\nks_table[\n    [\n        \"n_class_n\",\n        \"n_class_p\",\n    ]\n].plot.bar(stacked=True)\n\n<AxesSubplot:>\n\n\n\n\n\nThe graph above show the difference in classification for each decicles.\n\n\n\n\n\n\nNote\n\n\n\n\n\nThis difference varies depending on performance.\n\n\n\nKS 25 | \n\n\nKS 50 | \n\n\nKS 75 | \n\n\n\n\n\n\n\n\n\ncount_class_n = df.class_n.sum()\ncount_class_p = df.class_p.sum()\n\nks_table[\"%_class_n\"] = (ks_table.n_class_n / count_class_n) * 100\nks_table[\"%_class_p\"] = (ks_table.n_class_p / count_class_p) * 100\n\nks_table[\"cs_class_n\"] = (ks_table.n_class_n / count_class_n).cumsum() * 100\nks_table[\"cs_class_p\"] = (ks_table.n_class_p / count_class_p).cumsum() * 100\n\nks_table[\"cs_diff\"] = np.abs(ks_table[\"cs_class_n\"] - ks_table[\"cs_class_p\"])\nks_table.style.format(\n    \"{:.1f}%\", subset=[\"%_class_n\", \"%_class_p\", \"cs_class_n\", \"cs_class_p\", \"cs_diff\"]\n)\n\n\n\n\n  \n    \n      Â \n      min_score\n      max_score\n      n_class_n\n      n_class_p\n      n_total\n      %_class_n\n      %_class_p\n      cs_class_n\n      cs_class_p\n      cs_diff\n    \n  \n  \n    \n      0\n      0.035500\n      0.224967\n      21\n      12\n      33\n      13.3%\n      7.0%\n      13.3%\n      7.0%\n      6.3%\n    \n    \n      1\n      0.226262\n      0.327676\n      27\n      6\n      33\n      17.1%\n      3.5%\n      30.4%\n      10.5%\n      19.9%\n    \n    \n      2\n      0.328646\n      0.377685\n      19\n      14\n      33\n      12.0%\n      8.1%\n      42.4%\n      18.6%\n      23.8%\n    \n    \n      3\n      0.381554\n      0.437930\n      25\n      8\n      33\n      15.8%\n      4.7%\n      58.2%\n      23.3%\n      35.0%\n    \n    \n      4\n      0.442221\n      0.491976\n      23\n      10\n      33\n      14.6%\n      5.8%\n      72.8%\n      29.1%\n      43.7%\n    \n    \n      5\n      0.495512\n      0.575724\n      17\n      16\n      33\n      10.8%\n      9.3%\n      83.5%\n      38.4%\n      45.2%\n    \n    \n      6\n      0.577944\n      0.671211\n      9\n      24\n      33\n      5.7%\n      14.0%\n      89.2%\n      52.3%\n      36.9%\n    \n    \n      7\n      0.671344\n      0.757899\n      6\n      27\n      33\n      3.8%\n      15.7%\n      93.0%\n      68.0%\n      25.0%\n    \n    \n      8\n      0.765076\n      0.874251\n      4\n      29\n      33\n      2.5%\n      16.9%\n      95.6%\n      84.9%\n      10.7%\n    \n    \n      9\n      0.874542\n      0.988560\n      7\n      26\n      33\n      4.4%\n      15.1%\n      100.0%\n      100.0%\n      0.0%\n    \n  \n\n\n\n\n\n\n\n\n\nNote\n\n\n\n%_class_n for 0th bin is 21 / total number of class_n.\ncs_class_n is the same but cumulatively summed.\ncs_diff is the difference between cs_class_n and cs_class_p\n\n\n\n\n\nKS is the max difference between cumulatively summed rate of target and cumulatively summed non-target. i.e.Â maximum value of cs_diff\n\nks = ks_table[\"cs_diff\"].max()\n\nprint(f\"KS = {round(ks,2)}\")\n\nKS = 45.17\n\n\n\n\n\n\nfig, ax = plt.subplots(1, 1)\nax.plot(ks_table[\"cs_class_n\"], label=\"Class 0\")\nax.plot(ks_table[\"cs_class_p\"], label=\"Class 1\")\nks_tier = ks_table[\"cs_diff\"].idxmax()\nax.axvline(\n    x=ks_tier,\n    ymin=ks_table.loc[ks_tier, \"cs_class_n\"] / 100,\n    ymax=ks_table.loc[ks_tier, \"cs_class_p\"] / 100,\n    c=\"black\",\n    linewidth=1,\n    linestyle=\"--\",\n    label=f\"KS: {ks_table['cs_diff'].max().round(2)} at tier:{ks_tier}\",\n)\nax.set_xlim([0, 9])\nax.set_ylim([0, 100])\nax.set_xlabel(\"Tiers\")\nax.set_ylabel(\"Cumilative Percentage\")\nax.set_xticks(ks_table.index)\nax.grid()\nax.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nBinned KS are useful when you intent to have different actions for differnt bins. For example in the case of credit scoring you might give different APRs to customers falling in different bins."
  },
  {
    "objectID": "posts/bigquery-sessions/bigquery-sessions.html",
    "href": "posts/bigquery-sessions/bigquery-sessions.html",
    "title": "Bigquery Sessions",
    "section": "",
    "text": "Bigquery Session allows you to create variables, temporary table and table across multiple queries. Something like a ipython kernel. Allowing for REPL like experience.\nSesssion documentation\n\n\n\nfrom google.cloud import bigquery\n\ndef create_session(client) -> str:\n    return client.query(\n        \"SELECT 1;\", \n        job_config=bigquery.QueryJobConfig(create_session=True)\n    ).session_info.session_id\n\n\n\n\n\ndef query_in_session(client, session_id: str, query: str) \\\n    -> bigquery.QueryJob:\n    connection_properties = [\n        bigquery.query.ConnectionProperty(\n            key=\"session_id\", value=session_id\n        )\n    ]\n    job_config = bigquery.QueryJobConfig(\n        connection_properties=connection_properties\n    )\n    job = client.query(query, job_config=job_config)\n\n\n\n\n\n\n\nNote\n\n\n\nCreating a session creates a temporary dataset. You can find the temporary dataset id with the following. I am not sure if this is meant to found and/or if it is stable.\n\n\ntemporary dataset id\ndef _get_session_dataset_id() -> str:\n    job = query(\n        \"\"\"\n        CREATE TEMPORARY TABLE temp_table_fetch_dataset_id \n        AS (SELECT CURRENT_TIMESTAMP() as time);\n        \"\"\"\n    )\n    dataset_id = job.destination.dataset_id\n    self.query(f\"DROP TABLE temp_table_fetch_dataset_id\")\n    return dataset_id\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIt is also possible to attach a session to a existing dataset to a session. This allows the creation of permanent tables in sessions.\n\n\n\n\n\n\nfrom google.cloud import bigquery\nfrom google.cloud.bigquery import magics\n\nclient = bigquery.Client()\n\n# get session id\nsession_id = create_session(client)\n\n# change context's default query job config\nconnection_properties = [\n        bigquery.query.ConnectionProperty(\n            key=\"session_id\", value=session_id\n        )\n    ]\nmagics.context.default_query_job_config = bigquery.QueryJobConfig(\n    connection_properties=connection_properties\n)\n\n\n%load_ext google.cloud.bigquery\n\n\n%%bigquery\nSELECT CURRENT_TIMESTAMP();\n\n\n\n\n\ndef active_sessions(client):\n    \"\"\"Get the active sessions from client\"\"\"\n    project_id = client.project\n    location = client.location\n    query = f\"\"\"\n    SELECT\n    *\n    FROM\n        {project_id}.`region-{location.lower()}`.INFORMATION_SCHEMA.SESSIONS_BY_USER\n    WHERE \n        is_active = True\n    ORDER BY\n    creation_time DESC;\n    \"\"\"\n    job = client.query(query)\n    return job.to_dataframe()\n\n\n\n\n\n\nSession class\n# this is work in progress :)\nimport pandas as pd\n\nclass Session:\n    \"\"\"A BigQuery session connector.\"\"\"\n\n    def __init__(\n        self,\n        client: bigquery.Client,\n        session_id: str = None,\n        location: str = None,\n        dataset_id: str = None,\n        dataset_project_id: str = None,\n    ) -> None:\n        \"\"\"Construct instance.\"\"\"\n\n        # class variables\n        self.bigquery_client = client\n        self.project_id = client.project\n\n        # init location\n        if location is None:\n            self._location = client.location\n        else:\n            self._location = location\n\n        # init session_id\n        def _get_new_session_id() -> str:\n            job = self.bigquery_client.query(\n                \"SELECT 1;\",\n                job_config=bigquery.QueryJobConfig(create_session=True),\n                location=self._location,\n            )\n            return job.session_info.session_id\n\n        # get session id\n        if session_id is None:\n            self._session_id = _get_new_session_id()\n        else:\n            self._session_id = session_id\n            # check if session exists\n            job = self.query(\"SELECT 1;\")\n            job.result()\n\n        self.dataset_id = dataset_id  # TODO: if session_id already exists, check if dataset_id is the same\n        self.dataset_project_id = dataset_project_id\n\n        # set dataset_project_id\n        if dataset_project_id is not None:\n            self.query(f\"\"\"SET @@dataset_project_id = '{dataset_project_id}';\"\"\")\n        else:\n            self.query(f\"\"\"SET @@dataset_project_id = '{self.project_id}';\"\"\")\n\n        # set dataset_id\n        if dataset_id is not None:\n            self.query(f\"\"\"SET @@dataset_id = '{dataset_id}';\"\"\")\n        else:\n            # init session dataset id\n            def _get_session_dataset_id() -> str:\n                \"\"\"Get the dataset id of the session\"\"\"\n                job = self.query(\n                    \"\"\"\n                    CREATE TEMPORARY TABLE temp_table_fetch_dataset_id \n                    AS (SELECT CURRENT_TIMESTAMP() as time);\n                    \"\"\"\n                )\n                dataset_id = job.destination.dataset_id\n                self.query(f\"DROP TABLE temp_table_fetch_dataset_id\")\n                return dataset_id\n\n            # get the dataset id\n            self.dataset_id = _get_session_dataset_id()\n\n    def __del__(self) -> None:\n        \"\"\"Close the session when the instance is destroyed.\"\"\"\n        logging.debug(f\"Closing session {self._session_id}\")\n        if self._session_id:\n            job = self.bigquery_client.query(\n                f\"CALL BQ.ABORT_SESSION('{self._session_id}')\",\n                job_config=bigquery.QueryJobConfig(\n                    create_session=False,\n                    connection_properties=[\n                        bigquery.query.ConnectionProperty(\n                            key=\"session_id\", value=self._session_id\n                        )\n                    ],\n                ),\n                location=self._location,\n            )\n            job.result()\n        logging.debug(f\"Session {self._session_id} closed\")\n\n    def info(self) -> None:\n        \"\"\"Get info about the session\"\"\"\n        _str = \"\\n\".join(\n            [\n                f\"session_id: {self._session_id}\",\n                f\"project_id: {self.project_id}\",\n                f\"dataset_id: {self.dataset_id}\",\n                f\"location: {self._location}\",\n                f\"# of tables: {len(self.get_tables_meta())}\",\n            ]\n        )\n        print(_str)\n\n    def get_tables_meta(self) -> pd.DataFrame:\n        \"\"\"Get the tables metadata\"\"\"\n        query = f\"\"\"\n            SELECT \n                table_id,\n                TIMESTAMP_MILLIS(creation_time) as creation_time,\n                TIMESTAMP_MILLIS(last_modified_time) as last_modified_time,\n                row_count,\n                size_bytes,\n                type\n            FROM {self.dataset_id}.__TABLES__\n        \"\"\"\n        # TODO find out type : view or table\n        job = self.query(query)\n        return job.to_dataframe()\n\n    def query(self, query: str) -> bigquery.QueryJob:\n        \"\"\"Run a query in the session.\"\"\"\n        job = self.bigquery_client.query(\n            query,\n            job_config=bigquery.QueryJobConfig(\n                create_session=False,\n                connection_properties=[\n                    bigquery.query.ConnectionProperty(\n                        key=\"session_id\", value=self._session_id\n                    )\n                ],\n            ),\n            location=self._location,\n        )\n        job.result()  # wait for the job to complete\n        return job\n\n    def run(self, query: str) -> None:\n        \"\"\"Runs a query in session and returns nothing.\"\"\"\n        self.query(query)\n\n    def get_dataframe(self, query: str = None, table_name: str = None) -> pd.DataFrame:\n        \"\"\"Get a dataframe from a query or a table\"\"\"\n        if table_name is not None:\n            if query is not None:\n                raise ValueError(\"Only one of `table_name` or `query` can be specified\")\n            query = f\"SELECT * FROM {table_name}\"\n\n        # TODO: check if table memory  > 75% of local memory\n\n        job = self.query(query)\n        return job.to_dataframe()\n\n    def create_temp_table(\n        self, table_name: str, query: str, overwrite: bool = False\n    ) -> bigquery.TableReference:\n        \"\"\"Create a temporary table from a query\"\"\"\n\n        # TODO : check if table exists\n\n        # create table\n        edited_query = f\"\"\"CREATE TEMPORARY TABLE {table_name} AS ({query})\"\"\"\n        job = self.query(edited_query)\n\n        return job.destination\n\n    def drop_temp_table(self, table_name: str) -> None:\n        \"\"\"Destroy a temporary table\"\"\"\n        query = f\"\"\"DROP TABLE {table_name}\"\"\"\n        self.query(query)\n\n    def create_table(self, table_name: str, query: str) -> bigquery.TableReference:\n        \"\"\"Create a permanent table from a query\"\"\"\n        edited_query = f\"\"\"CREATE TABLE {table_name} AS ({query})\"\"\"\n        job = self.query(edited_query)\n        return job.destination\n\n    def drop_table(self, table_name: str) -> None:\n        \"\"\"Destroy a permanent table\"\"\"\n        query = f\"\"\"DROP TABLE {table_name}\"\"\"\n        self.query(query)\n\n    # aliases\n    get_df = get_dataframe"
  },
  {
    "objectID": "posts/mlflow/mlflow.html",
    "href": "posts/mlflow/mlflow.html",
    "title": "MLflow",
    "section": "",
    "text": "MLflow is an open source tool that manages machine learning workflows.\n\n\n\nTrack experiments by logging hyperparameters, parameters, metrics, etc.\nCollaborate on and compare experiments.\nShare data and models.\nEasier to deploy models - making the developement and production cycle tighter.\n\n\n\n\npip install mlflow\n\n\n\nmlflow server\nThis will launch an mlflow tracking server at http://127.0.0.1:5000. Which will record your experiments, runs and models.\n\n\n\n\n\n\nNote\n\n\n\nThis setup launches mlflow locally, to setup mlflow for a shared space follow the documentation\n\n\nif you open http://127.0.0.1:5000 in your browser, you would see the following.\n\n\n\n\n\n# sample dataset and model\n\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\n\nX, y = make_classification(n_samples=1000, class_sep=0.3, random_state=2)\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.33, random_state=0\n)\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\nprint(X_train[:2], \"\\n\", y_train[:2])\n\n(670, 20) (330, 20) (670,) (330,)\n[[-0.78669445  0.50422194  0.14499686  0.27023408 -0.41568319  2.29765363\n  -0.78492925  0.04416244  0.01998036 -1.56917287  1.63286779 -0.89135744\n   0.08933603 -0.80613711  0.05557609  1.36246637  2.43942101 -1.00591414\n  -1.80253155 -1.22509769]\n [ 0.7902646   1.58800122 -1.22745766  2.45455804  1.41597967 -0.9577786\n  -0.07197365  0.26854757 -0.13132486  1.30857502  0.28586077 -2.27043702\n   0.01217551  0.97624736  1.23833836 -0.19059581  0.05277705  0.45574733\n   0.23691436 -0.78250385]] \n [0 1]\n\n\n\nimport mlflow\n\n\n# connect to the mlflow server\nMLFLOW_TRACKING_URI = \"http://127.0.0.1:5000/\"\nmlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n\n\n# create experiment and get experiment ID\nexperiment_name = \"testing_mlflow\"\nexperiment_id = mlflow.create_experiment(experiment_name)\n\nif you go back to the mlflow page, you can see a new experiment created with the name.\n\n\n\nEvery experiment contains multiple runs. Each run start by mlflow.start_run()\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score\n\n\nwith mlflow.start_run(experiment_id=experiment_id):\n    \n    penalty = \"l2\"\n    C = 0.01\n    \n    clf = LogisticRegression(penalty=penalty, C=C)\n    clf.fit(X_train, y_train)\n    \n    train_auc = roc_auc_score(y_train, clf.predict_proba(X_train)[:,1])\n    test_auc = roc_auc_score(y_test, clf.predict_proba(X_test)[:,1])    \n    \n    mlflow.log_params({\"penalty\":penalty, \"C\":C}) # logs hyperparameters\n    mlflow.log_metrics({\"train_auc\":train_auc, \"test_auc\":test_auc}) # logs metrics\n\n\n# without `with` statement \nmlflow.start_run(experiment_id=experiment_id)\n\npenalty = \"l2\"\nC = 0.001\n\nclf = LogisticRegression(penalty=penalty, C=C)\nclf.fit(X_train, y_train)\n\ntrain_auc = roc_auc_score(y_train, clf.predict_proba(X_train)[:,1])\ntest_auc = roc_auc_score(y_test, clf.predict_proba(X_test)[:,1])    \n\nmlflow.log_params({\"penalty\":penalty, \"C\":C}) # logs hyperparameters\nmlflow.log_metrics({\"train_auc\":train_auc, \"test_auc\":test_auc}) # logs metrics\n\nmlflow.end_run() # be sure to end the run\n\nThe experiment should have new runs ready to be viewed.\n\n\nAny hyperparameter search can be logged to MLflow. (Including Optuna: Guide, MLflow-Optuna-Integration)\n\nfor c_value in [0.001, 0.01, 0.1, 1, 10]:\n    \n    with mlflow.start_run(experiment_id=experiment_id):\n    \n        penalty = \"l2\"\n        C = c_value\n\n        clf = LogisticRegression(penalty=penalty, C=C)\n        clf.fit(X_train, y_train)\n\n        train_auc = roc_auc_score(y_train, clf.predict_proba(X_train)[:,1])\n        test_auc = roc_auc_score(y_test, clf.predict_proba(X_test)[:,1])    \n\n        mlflow.log_params({\"penalty\":penalty, \"C\":C})\n        mlflow.log_metrics({\"train_auc\":train_auc, \"test_auc\":test_auc}) \n\n\n\n\n\nruns = mlflow.search_runs(experiment_id) # returns a dataframe\n\nprint(f\"{type(runs) = }\") \nruns\n\ntype(runs) = <class 'pandas.core.frame.DataFrame'>\n\n\n\n\n\n\n  \n    \n      \n      run_id\n      experiment_id\n      status\n      artifact_uri\n      start_time\n      end_time\n      metrics.train_auc\n      metrics.test_auc\n      params.penalty\n      params.C\n      tags.mlflow.source.type\n      tags.mlflow.user\n      tags.mlflow.source.name\n    \n  \n  \n    \n      0\n      c6cc6209735d49d8ba344db544de8c6f\n      1\n      FINISHED\n      ./mlruns/1/c6cc6209735d49d8ba344db544de8c6f/ar...\n      2022-05-17 03:18:53.686000+00:00\n      2022-05-17 03:18:53.711000+00:00\n      0.684686\n      0.649352\n      l2\n      10\n      LOCAL\n      Work\n      /Users/Work/anaconda3/envs/mlogs/lib/python3.8...\n    \n    \n      1\n      fd14937f4ba14e4f99b397a7fd911b15\n      1\n      FINISHED\n      ./mlruns/1/fd14937f4ba14e4f99b397a7fd911b15/ar...\n      2022-05-17 03:18:53.652000+00:00\n      2022-05-17 03:18:53.678000+00:00\n      0.684686\n      0.649352\n      l2\n      1\n      LOCAL\n      Work\n      /Users/Work/anaconda3/envs/mlogs/lib/python3.8...\n    \n    \n      2\n      e68fa76fe03a41d9a61c0932986208df\n      1\n      FINISHED\n      ./mlruns/1/e68fa76fe03a41d9a61c0932986208df/ar...\n      2022-05-17 03:18:53.619000+00:00\n      2022-05-17 03:18:53.646000+00:00\n      0.684596\n      0.650862\n      l2\n      0.1\n      LOCAL\n      Work\n      /Users/Work/anaconda3/envs/mlogs/lib/python3.8...\n    \n    \n      3\n      5ad09dce7f784075880caa9db33b6c18\n      1\n      FINISHED\n      ./mlruns/1/5ad09dce7f784075880caa9db33b6c18/ar...\n      2022-05-17 03:18:53.589000+00:00\n      2022-05-17 03:18:53.613000+00:00\n      0.684177\n      0.657014\n      l2\n      0.01\n      LOCAL\n      Work\n      /Users/Work/anaconda3/envs/mlogs/lib/python3.8...\n    \n    \n      4\n      a659f2aff5904a38bf55582f356a6a34\n      1\n      FINISHED\n      ./mlruns/1/a659f2aff5904a38bf55582f356a6a34/ar...\n      2022-05-17 03:18:53.553000+00:00\n      2022-05-17 03:18:53.582000+00:00\n      0.681760\n      0.668435\n      l2\n      0.001\n      LOCAL\n      Work\n      /Users/Work/anaconda3/envs/mlogs/lib/python3.8...\n    \n    \n      5\n      ac488b27fd284e63b0fd09777a322726\n      1\n      FINISHED\n      ./mlruns/1/ac488b27fd284e63b0fd09777a322726/ar...\n      2022-05-17 03:18:52.679000+00:00\n      2022-05-17 03:18:52.708000+00:00\n      0.681760\n      0.668435\n      l2\n      0.001\n      LOCAL\n      Work\n      /Users/Work/anaconda3/envs/mlogs/lib/python3.8...\n    \n    \n      6\n      767e1f0fdaa644d5bd478c5899fb3ddc\n      1\n      FINISHED\n      ./mlruns/1/767e1f0fdaa644d5bd478c5899fb3ddc/ar...\n      2022-05-17 03:18:52.442000+00:00\n      2022-05-17 03:18:52.475000+00:00\n      0.684177\n      0.657014\n      l2\n      0.01\n      LOCAL\n      Work\n      /Users/Work/anaconda3/envs/mlogs/lib/python3.8...\n    \n  \n\n\n\n\n\n\n\n\n\nwith mlflow.start_run(experiment_id=experiment_id) :\n\n    penalty = \"l2\"\n    C = 0.001\n\n    clf = LogisticRegression(penalty=penalty, C=C)\n    clf.fit(X_train, y_train)\n\n    train_auc = roc_auc_score(y_train, clf.predict_proba(X_train)[:,1])\n    test_auc = roc_auc_score(y_test, clf.predict_proba(X_test)[:,1])    \n\n    mlflow.log_params({\"penalty\":penalty, \"C\":C}) \n    mlflow.log_metrics({\"train_auc\":train_auc, \"test_auc\":test_auc}) \n    \n    mlflow.sklearn.log_model(clf, \"log_res\") # logs model with model name\n\n/Users/Work/anaconda3/envs/mlogs/lib/python3.8/site-packages/_distutils_hack/__init__.py:30: UserWarning: Setuptools is replacing distutils.\n  warnings.warn(\"Setuptools is replacing distutils.\")\n\n\nThis save the model file (in pickle format) and also saves conda.yaml file that contains the environment requirements and MLmodel which is a yaml file that contains instructions for MLflow to run this model.\n\n\n\n\nlogged_model = \"runs:/29b59f6a1ba84859b299569cc3f1b004/log_res\"\nloaded_model = mlflow.pyfunc.load_model(logged_model)\n\n\nloaded_model.predict(X_test)[:5]\n\narray([1, 0, 1, 0, 0])\n\n\nMLflowâ€™s API only contains predict function but it is possible for that work as predict_proba.\nclass SklearnModelWrapper(mlflow.pyfunc.PythonModel):\n  def __init__(self, model):\n    self.model = model\n    \n  def predict(self, context, model_input):\n    return self.model.predict_proba(model_input)[:,1]\nrefer here\n\n\n\n\n\n\nNote\n\n\n\nIf the mlflow is in a shared space, it is possible for anyone having access to the mlflow server to load the model easily. This is one of the hallmarks of mlflow.\n\n\n\n\n\n\nArtifacts are any file or directory that you would like to record as part of a run. Artifacts can be logged as any formats.\nFor example : - graphs ( as images )\n- tables/dataframe ( as html tables df.to_html() ) - datasets (csvs, parquets) - feature list (txt) - git-commit ids\n\nfrom pathlib import Path\n\nimport pandas as pd\n\ndata_dir = str(Path.cwd()) + \"/data\"\nPath(data_dir).mkdir(exist_ok=True)\n\npd.DataFrame(X_train).to_csv(f\"{data_dir}/X_train.csv\", index=False)\npd.DataFrame(X_test).to_csv(f\"{data_dir}/X_test.csv\", index=False)\n\npd.DataFrame(y_train).to_csv(f\"{data_dir}/y_train.csv\", index=False)\npd.DataFrame(y_test).to_csv(f\"{data_dir}/y_test.csv\", index=False)\n\n\nwith mlflow.start_run(experiment_id=experiment_id) :\n\n    penalty = \"l2\"\n    C = 0.001\n\n    clf = LogisticRegression(penalty=penalty, C=C)\n    clf.fit(X_train, y_train)\n\n    train_auc = roc_auc_score(y_train, clf.predict_proba(X_train)[:,1])\n    test_auc = roc_auc_score(y_test, clf.predict_proba(X_test)[:,1])    \n\n    mlflow.log_params({\"penalty\":penalty, \"C\":C}) \n    mlflow.log_metrics({\"train_auc\":train_auc, \"test_auc\":test_auc}) \n    \n    mlflow.sklearn.log_model(clf, \"log_res\")\n    \n    mlflow.log_artifacts(data_dir, artifact_path=\"data\") # logs the entire directory\n\n\n\n\n\n\n\nNote\n\n\n\nThis will copy all the contents of the directory to mlflow server. If the server is in a shared space, anyone having access can now download the artifacts for experimentation or validation.\n\n\n\n\n\n# downloads the `data` artifact from the previously recorded run\nfrom mlflow.tracking import MlflowClient\n\nclient = MlflowClient()\n\ndownload_dir = str(Path.cwd()) + \"/data/download\"\nPath(download_dir).mkdir(exist_ok=True)\n\nartifact_path = client.download_artifacts(\n    \"1694c57a0b674342b88270b49702e551\", \"data\", download_dir\n)\n\n\nX_train_downloaded = pd.read_csv(f\"{artifact_path}/X_train.csv\")\nprint(X_train_downloaded.shape)\n\n(670, 20)\n\n\n\n\n\n\nRegistering model requires the MLflow server to be hosted with some kind of RDS for model registry data storage. Registering model is similar to logging them with the difference of giving it a name and version. Registered models go into the model registry and then can be easily deployed."
  },
  {
    "objectID": "posts/conda/conda.html",
    "href": "posts/conda/conda.html",
    "title": "Conda",
    "section": "",
    "text": "github\n\n\n\nEnvironment Manager Every project has itâ€™s own dependencies. One project might use Tensorflow and another might use XGBoost. There is no reason why these two project should be in the same enviroment. Conda lets you create different enviroments for every project ( or every type of project )\nShare your environments If you are working with a team, it is essentials that you develop on the same enviroment so as to avoid dependency issues. You would not only be tracking your python package dependencies but also python version itself. This sharing also applied to building projects in production, which can use the same enviroment.\n\n\n\n\n\n1. Download Installer (for Linux)\nwget https://repo.anaconda.com/archive/Anaconda3-2021.11-Linux-x86_64.sh\n2. Install\nbash ./Anaconda3-2021.11-Linux-x86_64.sh\n3. Follow the instruction prompt.\nChoose the installtion location depending on storage space. Conda enviroment tend to take up to few GBs per enviroment\n\n\n\n\n\n\nTip\n\n\n\nWhen installer prompts â€œDo you wish the installer to initialize Anaconda3 by running conda init?â€ Choose â€œyesâ€. This will start conda with every terminal session.\n\n\n4. Restart the terminal or enter the following\nsource ~/.bashrc\nyou should see (base) in your bash prompt\n\n\n\n\n\n\nWhen conda is installed you get a default environment called base\n\n\nconda activate base \nActivates the base environment. You should see (base) in bash prompt.\nwhich python\nthis should point to base environmentâ€™s python. For example : ~/anaconda3/bin/python\n\n\n\nconda deactivate && which python\nthis deactivates condaâ€™s base env and now â€˜pythonâ€™ should point to : /usr/bin/python\n\n\n\n\n\n\nNote\n\n\n\nThis applies to pip as well. So packages installed through pip inside the base environment will not be avaiable in the systemâ€™s python ( after deactivating ). try â€˜which pipâ€™ inside and outside the base env\n\n\n\n\n\n\n\n\n\n\nconda create -n custom_env python=3.8.2\n\n\n\n\n\n\nNote\n\n\n\ncustom_env is the name of the environment.\n-n is short hand for --name\n\n\n\n\n\nconda info --envs\n\n\n\nconda activate custom_env\n\n\n\n\n\n\nNote\n\n\n\ntry which python and python --version after this step. It should point to this environmentâ€™s python.\nconda deactivate will take you to base environment.\n\n\n\n\n\nconda env export > custom_env_file.yml\nThis file can be inside your project and can be version controlled via git.\n\n\n\nconda remove -n custom_env --all \nmake sure to deactivate the environment before removing the environment\n\n\n\nconda env create -f custom_env_file.yml \nit will create custom_env. The name of the environment is also stored in the file.\n\n\n\n\n\n\nTip\n\n\n\nLook at the file after installing some package with pip and conda. You need to export to see the changes.\n\n\n\n\n\n\nYou can also use conda to install packages. These packages will be install from the default anaconda channels\nconda install pandas \nwill install pandas similar to pip install pandas\n\n\n\n\n\n\nNote\n\n\n\nNot all packages are avaiable in conda but you can use pip inside the environment and it will be tracked conda dependencies. Most pip package are also avaiable through conda-forge an open-source channel for conda.conda install -c conda-forge optuna\n\n\n\n\n\ncondaâ€™s cheet sheet (from condaâ€™s doc)"
  },
  {
    "objectID": "posts/decision-trees/decision_tree.html",
    "href": "posts/decision-trees/decision_tree.html",
    "title": "Decision Trees",
    "section": "",
    "text": "A non-parametric supervised learning method. Decision tree tries to predict the target variable with tree of simple decision rules. It is used for both Regression and Classification.\n\n\n\nEasy to interpret\nSupports categorical and continuous features\n\n\n\n\nTree is an abstract idea to store information i.e an abstact data type. It symbolizes a hierarchial graph that contains parent and child nodes, where a parent node can have multiple child nodes but a child node has only one parent node. The tree starts at a root node which does not have any parent node and end at leaf nodes, which do not have any child nodes.\n\n\n\n\n\n\n\nNote\n\n\n\nTree can have more than 2 child nodes for a parent node, but for the purposes of computation and understanding the tree in decision tree is limited to 2 child nodes for every parent node (a.k.a Binary Tree)\n\n\n\n\n\n\nEvery node receives data in the form of {features, target}.\nWe then as a question to a feature, the answer to which - splits this set into two.\nThe question asked is determined by Attribute Selection Measure - which is heuristic for measuring the understanding of the target given the split.\nWe start the root node with all of the training set.\nWe end when we cannot split (due to all leaf nodes being pure or some condition by hyperparameters like maximum depth ) or have run out of features.\n\nThere are varations in types of decision tree algorithms, but generally they follow these steps\n\n\nThese are metrics that measure the quality of a split.\n\nInformation Gain & Entropy\n\nGini Impurity\n\n\n\nInformation gain is based on entropy (from information theory)\n\n\nEntropy is defined as\n\\[E = -\\sum_{i=1}^{J} p_i log_2 p_i\\]\nwhere\n\\(i\\) is each class in a node\n\\(J\\) is thetotal number of classes\n\\(p_i\\) is the probablity of class \\(i\\) and Sum of \\(p1, p2 â€¦ pJ\\) equals to 1. Entropy ranges from 0 to 1.\nConsider the following example.\n\nif the split is at \\( x = 2 \\),\nthen entropy for the left branch i.e \\( x < 2 \\) is\n\\[\nE = - (p_{square} log_2 p_{square} + p_{circle} log_2 p_{circle}) \\\\\n= -(  \\frac{5}{7} log_2 \\frac{5}{7} + \\frac{2}{7} log_2 \\frac{2}{7}  ) \\\\\n= 0.86\n\\]\nEntropy for the left branch i.e \\( x > 2 \\) is\n\\[\nE = - p_{circle} log_2 p_{circle} \\\\\n= - \\frac{5}{5} log_2 \\frac{5}{5}  \\\\\n= 0\n\\]\n\n\n\nInformation Gain is defined as\n\\[ IG = E(before) - \\sum_{i=1}^J E(i, after)  \\]\nSplit are made to maximize information gain.\nFor the example above,\n\\[ E(before) = - (p_{square} log_2 p_{square} + p_{circle} log_2 p_{circle}) \\\\\n= -(  \\frac{5}{12} log_2 \\frac{5}{12} + \\frac{7}{12} log_2 \\frac{7}{12}  ) \\\\ = 0.97 \\]\n\\[ \\sum_{i=1}^J E(i, after)   = 0.86 + 0 \\\\ = 0.86 \\]\n\\[ IG = 0.97 - 0.86 = 0.11 \\]\nhere is another example :\n\n\n\n\n\nChances of being incorrect if randomly assigned a class (accounting class distribution). Gini Impurity is defined as\n\\[ G = 1 - \\sum_{i=1}^J (p_i)^2 \\]\nwhere\n\\(i\\) is each class in a node\n\\(J\\) is the total number of classes\n\\(p_i\\) is the probablity of class \\(i\\)\n\n\n\n\n\n\nNote\n\n\n\nGini impurity is the preferred because it doesnâ€™t require logarithmic computation which is relatively more expensive. The tree produced by both methods tend to be similar. refer here\n\n\nFor example,\n\nif the split is at \\( x = 2 \\),\nthen gini for the left branch i.e \\( x < 2 \\) is \\[\nG_{left} = 1 - (p_{square}^2 + p_{circle}^2) \\\\\n= 1 - (  ({\\frac{5}{7}})^2 + (\\frac{2}{7})^2  ) \\\\\n= 0.40\n\\]\nthen gini for the right branch i.e \\( x > 2 \\) is \\[\nG_{right} = 1 - ( p_{circle}^2) \\\\\n= 0\n\\]\n\n\n\n\n\n\nNote\n\n\n\nSimilar to information gain, the split deciced by the maximum difference in the gini impurity (the amount of impurity removed: before split - after split ). It is also weighted by the number of elements in each splity."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "mlogs",
    "section": "",
    "text": "Bigquery Sessions\n\n\n\nbigquery\n\n\npython\n\n\ndata-engineering\n\n\n\n\nNov 28, 2022\n\n\n\n\n\n12/2/22, 1:05:53 PM\n\n\n\n\n\n\n\n\n\n\nMemory Profiler\n\n\n\nmemory\n\n\npython\n\n\n\n\nNov 2, 2022\n\n\n\n\n\n12/2/22, 1:05:53 PM\n\n\n\n\n\n\n\n\n\n\nDecision Trees\n\n\n\nml\n\n\n\n\nJun 13, 2022\n\n\n\n\n\n12/2/22, 1:05:53 PM\n\n\n\n\n\n\n\n\n\n\nMLflow\n\n\n\npython\n\n\nml\n\n\n\n\nJun 1, 2022\n\n\n\n\n\n12/2/22, 1:05:53 PM\n\n\n\n\n\n\n\n\n\n\nOptuna\n\n\n\nhyperparameter\n\n\ntuning\n\n\npython\n\n\n\n\nMay 30, 2022\n\n\n\n\n\n12/2/22, 1:05:53 PM\n\n\n\n\n\n\n\n\n\n\nConda\n\n\n\nenvironment\n\n\npython\n\n\n\n\nMay 15, 2022\n\n\n\n\n\n12/2/22, 1:05:53 PM\n\n\n\n\n\n\n\n\n\n\nKS Metric\n\n\n\nmetrics\n\n\nml\n\n\npython\n\n\n\n\nApr 28, 2022\n\n\n\n\n\n12/2/22, 1:05:53 PM\n\n\n\n\n\n\n\nNo matching items"
  }
]