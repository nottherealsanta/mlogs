{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3168d2ff-3e2e-4fe6-9ab9-bd62251eff12",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "A non-parametric supervised learning method. Decision tree tries to predict the target variable with tree of simple decision rules. It is used for both Regression and Classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7d493f-978a-4176-b781-05d5df05a4be",
   "metadata": {},
   "source": [
    "## Why Decision Trees ?\n",
    "    \n",
    "- Easy to interpret\n",
    "- Supports categorical and continuous features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b427979",
   "metadata": {},
   "source": [
    "## Tree\n",
    "\n",
    "Tree is an abstract idea to store information i.e an abstact data type. It symbolizes a hierarchial graph that contains parent and child nodes, where a parent node can have multiple child nodes but a child node has only one parent node. The tree starts at a root node which does not have any parent node and end at leaf nodes, which do not have any child nodes. \n",
    "\n",
    "![](assets/trees.png)\n",
    "\n",
    "```{note}\n",
    "Tree can have more than 2 child nodes for a parent node, but for the purposes of computation and understanding the tree in decision tree is limited to 2 child nodes for every parent node (a.k.a Binary Tree)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037a16ae-5a09-4410-8099-d0a917ea9335",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Inner workings of Decision Tree\n",
    "\n",
    "- Every node receives data in the form of {features, target}. \n",
    "- We then as a question to a feature, the answer to which - splits this set into two.\n",
    "- The question asked is determined by `Attribute Selection Measure` - which is heuristic for measuring the understanding of the target given the split.\n",
    "- We start the root node with all of the training set.\n",
    "- We end when we cannot split (due to all leaf nodes being pure or some condition by hyperparameters like maximum depth ) or have run out of features. \n",
    "\n",
    "There are varations in types of decision tree algorithms, but generally they follow these steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9f94a8-f519-4b1e-804d-1d66ce863677",
   "metadata": {},
   "source": [
    "### Attribute Selection Measures\n",
    "\n",
    "These are metrics that measure the quality of a split.\n",
    "\n",
    "- [Information Gain & Entropy](####Information-gain-&-Entropy)  \n",
    "- [Gini Impurity ](####Gini-Impurity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd66255-339c-43a5-a692-4d336058d4bc",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Information Gain & Entropy\n",
    "\n",
    "Information gain is based on entropy (from [information theory](https://en.wikipedia.org/wiki/Information_theory))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a260093d-e012-4bbd-b664-1b951aed9eb0",
   "metadata": {},
   "source": [
    "##### Entropy\n",
    "\n",
    "Entropy is defined as\n",
    "\n",
    "$$E = -\\sum_{i=1}^{J} p_i log_2 p_i$$\n",
    "\n",
    "where  \n",
    "\\\\(i\\\\) is each class in a node  \n",
    "\\\\(J\\\\) is thetotal number of classes  \n",
    "\\\\(p_i\\\\) is the probablity of class \\\\(i\\\\) \n",
    "and Sum of \\\\(p1, p2 ... pJ\\\\) equals to 1. Entropy ranges from 0 to 1.\n",
    "\n",
    "Consider the following example. \n",
    "\n",
    "```{image} assets/dt_example_1.png\n",
    ":class: bg-primary\n",
    ":width: 300px\n",
    ":align: center\n",
    "```\n",
    "\n",
    "if the split is at \\\\( x  = 2 \\\\),  \n",
    "then entropy for the left branch _i.e_ \\\\( x < 2 \\\\) is\n",
    "\n",
    "$$ \n",
    " E = - (p_{square} log_2 p_{square} + p_{circle} log_2 p_{circle}) \\\\\n",
    "= -(  \\frac{5}{7} log_2 \\frac{5}{7} + \\frac{2}{7} log_2 \\frac{2}{7}  ) \\\\\n",
    "= 0.86\n",
    "$$\n",
    "\n",
    "Entropy for the left branch _i.e_ \\\\( x > 2 \\\\) is\n",
    "\n",
    "$$ \n",
    " E = - p_{circle} log_2 p_{circle} \\\\\n",
    "= - \\frac{5}{5} log_2 \\frac{5}{5}  \\\\\n",
    "= 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714e97e6-106b-4b14-84d1-1edc8168e0f2",
   "metadata": {},
   "source": [
    "##### Information Gain\n",
    "\n",
    "Information Gain is defined as \n",
    "\n",
    "$$ IG = E(before) - \\sum_{i=1}^J E(i, after)  $$\n",
    "\n",
    "Split are made to maximize information gain.\n",
    "\n",
    "For the example above, \n",
    "\n",
    "$$ E(before) = - (p_{square} log_2 p_{square} + p_{circle} log_2 p_{circle}) \\\\\n",
    "= -(  \\frac{5}{12} log_2 \\frac{5}{12} + \\frac{7}{12} log_2 \\frac{7}{12}  ) \\\\ = 0.97 $$\n",
    "\n",
    "$$ \\sum_{i=1}^J E(i, after)   = 0.86 + 0 \\\\ = 0.86 $$\n",
    "\n",
    "$$ IG = 0.97 - 0.86 = 0.11 $$\n",
    "\n",
    "here is another example :\n",
    "```{image} assets/dt_example_2.png\n",
    ":class: bg-primary\n",
    ":width: 600px\n",
    ":align: center\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39de08dd-abd0-4d9d-99ed-545fc9652ae4",
   "metadata": {},
   "source": [
    "#### Gini Impurity\n",
    "\n",
    "Chances of being incorrect if randomly assigned a class (accounting class distribution). Gini Impurity is defined as\n",
    "\n",
    "$$ G = 1 - \\sum_{i=1}^J (p_i)^2 $$\n",
    "\n",
    "where  \n",
    "\\\\(i\\\\) is each class in a node  \n",
    "\\\\(J\\\\) is the total number of classes  \n",
    "\\\\(p_i\\\\) is the probablity of class \\\\(i\\\\)\n",
    "\n",
    "```{note}\n",
    "Gini impurity is the preferred because it doesn't require logarithmic computation which is relatively more expensive. The tree produced by both methods tend to be similar. [refer here](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/decision-tree-binary.md)\n",
    "```\n",
    "\n",
    "For example,\n",
    "\n",
    "```{image} assets/dt_example_1.png\n",
    ":class: bg-primary\n",
    ":width: 300px\n",
    ":align: center\n",
    "```\n",
    "if the split is at \\\\( x  = 2 \\\\),  \n",
    "then gini for the left branch _i.e_ \\\\( x < 2 \\\\) is\n",
    "$$ \n",
    "G_{left} = 1 - (p_{square}^2 + p_{circle}^2) \\\\\n",
    "= 1 - (  ({\\frac{5}{7}})^2 + (\\frac{2}{7})^2  ) \\\\\n",
    "= 0.40\n",
    "$$\n",
    "\n",
    "then gini for the right branch _i.e_ \\\\( x > 2 \\\\) is\n",
    "$$ \n",
    "G_{right} = 1 - ( p_{circle}^2) \\\\\n",
    "= 0\n",
    "$$\n",
    "\n",
    "```{note}\n",
    "Similar to information gain, the split deciced by the maximum difference in the gini impurity (_the amount of impurity removed: before split - after split_ ). It is also weighted by the number of elements in each splity.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdda449a-e654-4b13-8d5e-e9934b751c01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
